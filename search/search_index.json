{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Once the model is trained, we want to verify our model as quickly as possible. nlp2go provides a CLI interface and Restful api that allows you to quickly deploy model to everyone. Feature \u00b6 There are many additional features : - Support loading multiple models at a time. - Provide input format checking. - You can also load models in python code. - Flexible handling of parameters so that parameter can be changed in each prediction. - Support huggingface transformers\u2019s model. - There are models in the Model Hub for you to try. Quick Start \u00b6 Installing via pip \u00b6 pip install nlp2go Load model into a CLI interface \u00b6 nlp2go --model model_path --cli Load model into a Restful API \u00b6 nlp2go --model model_path --api_path model_task_name --api_port 3000 You can also try nlp2go in Google Colab: Contributing \u00b6 Thanks for your interest.There are many ways to contribute to this project. Get started here . License \u00b6 License","title":"Home"},{"location":"#introduction","text":"Once the model is trained, we want to verify our model as quickly as possible. nlp2go provides a CLI interface and Restful api that allows you to quickly deploy model to everyone.","title":"Introduction"},{"location":"#feature","text":"There are many additional features : - Support loading multiple models at a time. - Provide input format checking. - You can also load models in python code. - Flexible handling of parameters so that parameter can be changed in each prediction. - Support huggingface transformers\u2019s model. - There are models in the Model Hub for you to try.","title":"Feature"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installing-via-pip","text":"pip install nlp2go","title":"Installing via pip"},{"location":"#load-model-into-a-cli-interface","text":"nlp2go --model model_path --cli","title":"Load model into a CLI interface"},{"location":"#load-model-into-a-restful-api","text":"nlp2go --model model_path --api_path model_task_name --api_port 3000 You can also try nlp2go in Google Colab:","title":"Load model into a Restful API"},{"location":"#contributing","text":"Thanks for your interest.There are many ways to contribute to this project. Get started here .","title":"Contributing"},{"location":"#license","text":"License","title":"License"},{"location":"class/","text":"\u00b6 main () \u00b6 loading argument and starting an model interface Source code in nlp2go/main.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def main (): \"\"\" loading argument and starting an model interface \"\"\" global model_dict model_dict = defaultdict ( dict ) parser = argparse . ArgumentParser () group = parser . add_mutually_exclusive_group ( required = True ) group . add_argument ( '--json' , type = str ) group . add_argument ( '--model' , type = str ) parser . add_argument ( '--enable_arg_panel' , action = 'store_true' , help = 'enable panel to input argument' ) parser . add_argument ( '--task' , type = str ) # interface parser . add_argument ( '--api_path' , type = str , default = \"model\" , help = 'api path to serve the demo' ) parser . add_argument ( '--api_port' , type = int , default = 3000 , help = 'port to serve the demo on' ) parser . add_argument ( '--cli' , action = 'store_true' , help = 'commandline mode' ) args = parser . parse_args () if args . model : model_dict [ args . api_path ] = Model ( args . model , args . task , args . enable_arg_panel ) else : with open ( args . json , 'r' , encoding = 'utf8' ) as reader : model_dict = json . loads ( reader . read ()) for path , d in model_dict . items (): task = model_dict [ path ][ 'task' ] if 'task' in model_dict [ path ] else None model = Model ( model_dict [ path ][ 'model' ], task , args . enable_arg_panel ) model_dict [ path ] = model if args . cli : cli = Cli () cli . start ( model_dict ) else : server = Server () server . start ( model_dict , args . api_port ) \u00b6 \u00b6 \u00b6 \u00b6","title":"Class"},{"location":"class/#nlp2go.main","text":"","title":"nlp2go.main"},{"location":"class/#nlp2go.main.main","text":"loading argument and starting an model interface Source code in nlp2go/main.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def main (): \"\"\" loading argument and starting an model interface \"\"\" global model_dict model_dict = defaultdict ( dict ) parser = argparse . ArgumentParser () group = parser . add_mutually_exclusive_group ( required = True ) group . add_argument ( '--json' , type = str ) group . add_argument ( '--model' , type = str ) parser . add_argument ( '--enable_arg_panel' , action = 'store_true' , help = 'enable panel to input argument' ) parser . add_argument ( '--task' , type = str ) # interface parser . add_argument ( '--api_path' , type = str , default = \"model\" , help = 'api path to serve the demo' ) parser . add_argument ( '--api_port' , type = int , default = 3000 , help = 'port to serve the demo on' ) parser . add_argument ( '--cli' , action = 'store_true' , help = 'commandline mode' ) args = parser . parse_args () if args . model : model_dict [ args . api_path ] = Model ( args . model , args . task , args . enable_arg_panel ) else : with open ( args . json , 'r' , encoding = 'utf8' ) as reader : model_dict = json . loads ( reader . read ()) for path , d in model_dict . items (): task = model_dict [ path ][ 'task' ] if 'task' in model_dict [ path ] else None model = Model ( model_dict [ path ][ 'model' ], task , args . enable_arg_panel ) model_dict [ path ] = model if args . cli : cli = Cli () cli . start ( model_dict ) else : server = Server () server . start ( model_dict , args . api_port )","title":"main()"},{"location":"class/#nlp2go.model.Model","text":"","title":"nlp2go.model.Model"},{"location":"class/#nlp2go.parser","text":"","title":"nlp2go.parser"},{"location":"class/#nlp2go.cli","text":"","title":"nlp2go.cli"},{"location":"class/#nlp2go.server","text":"","title":"nlp2go.server"},{"location":"installation/","text":"Installation \u00b6 nlp2go is tested on Python 3.6+, and PyTorch 1.1.0+. Installing via pip \u00b6 pip install nlp2go Installing via source \u00b6 git clone https://github.com/voidful/nlp2go.git python setup.py install Running nlp2go \u00b6 Once you've installed nlp2go, you can run with pip installed version: \u00b6 nlp2go local version: \u00b6 python -m nlp2go.main","title":"Installation"},{"location":"installation/#installation","text":"nlp2go is tested on Python 3.6+, and PyTorch 1.1.0+.","title":"Installation"},{"location":"installation/#installing-via-pip","text":"pip install nlp2go","title":"Installing via pip"},{"location":"installation/#installing-via-source","text":"git clone https://github.com/voidful/nlp2go.git python setup.py install","title":"Installing via source"},{"location":"installation/#running-nlp2go","text":"Once you've installed nlp2go, you can run with","title":"Running nlp2go"},{"location":"installation/#pip-installed-version","text":"nlp2go","title":"pip installed version:"},{"location":"installation/#local-version","text":"python -m nlp2go.main","title":"local version:"},{"location":"models_hub/","text":"","title":"Models hub"},{"location":"usage/","text":"Usage \u00b6 Overview \u00b6 $ nlp2go arguments: either json config or model path --model model path or --json json file include models setting optional arguments: -h, --help show this help message and exit --enable_arg_panel enable argument input panel --path restful api path --port restful api hosting port --cli enable command line interface(default using restful api) Load Model \u00b6 load TFkit models \u00b6 online model \u00b6 nlp2go --model model_name_from_models_hub --cli local model \u00b6 nlp2go --model model_path.pt --cli load Huggingface's transformers models \u00b6 online model \u00b6 nlp2go --model model_name_from_https://huggingface.co --task any of it:[feature-extraction, sentiment-analysis, ner, question-answering, fill-mask, summarization, translation_en_to_fr, translation_en_to_de, translation_en_to_ro, text-generation] --cli example: nlp2go --model voidful/albert_chinese_tiny --task fill-mask --cli local model \u00b6 nlp2go --model model_dir --task any of it:[feature-extraction, sentiment-analysis, ner, question-answering, fill-mask, summarization, translation_en_to_fr, translation_en_to_de, translation_en_to_ro, text-generation] --cli load multiple models \u00b6 For deploying multiple model to restful api, nlp2go needs to load models from a json config file. nlp2go --json json_config_path --cli json format: { \"API1_PATH\" : { \"model\" : \"model1_path or name\" , \"model_parameter\" : \"argument\" }, \"API2_PATH\" : { \"model\" : \"model1_path or name\" , \"model_parameter\" : \"argument\" } } Example: ./test_conf.json { \"albert_mask\" : { \"model\" : \"voidful/albert_chinese_tiny\" , \"task\" : \"fill-mask\" } } run nlp2go --json ./test_conf.json --api_port 3000 result hosting api in path: /api/+ ['albert_mask'] Model loaded, serving demo on port 3000 test Get: localhost:3000/api/albert_mask?input=\u4eca\u5929[MASK]\u60c5\u5f88\u597d pre-load models \u00b6 nlp2go-preload is to pre-download model files when we host our models using docker. that can avoid downloading model at run time nlp2go-preload --json config.json example dockerfile FROM pytorch/pytorch:latest RUN apt-get -y update # Create app directory RUN mkdir -p /usr/src/app WORKDIR /usr/src/app # Copy source project COPY . /usr/src/app/ RUN pip install nlprep tfkit nlp2go -U RUN nlp2go-preload --json config.json CMD PYTHONIOENCODING = utf8 nlp2go --json config.json --port ${ PORT } Model Interface \u00b6 Command Line Interface \u00b6 model predict using command line interface(cli) nlp2go --model voidful/albert_chinese_tiny --task fill-mask --cli argument: --enable_arg_panel to enable a input panel that let you input argument Restful API \u00b6 nlp2go --model voidful/albert_chinese_tiny --task fill-mask --api_path model --api_port 3000 argument: --enable_arg_panel to enable a input panel that let you input argument --api_path path set API path after 'host/API/' --api_port set hosting port Python code \u00b6 import nlp2go albert_fillmask = nlp2go . Model ( model_path = \"voidful/albert_chinese_tiny\" , model_task = \"fill-mask\" ) albert_fillmask . predict ({ \"input\" : \"\u4eca\u5929[MASK]\u60c5\u5f88\u597d\" }) result { 'result': [[{'sequence': '[CLS] \u4eca \u5929 \u611f \u60c5 \u5f88 \u597d [SEP]', 'score': 0.40312328934669495, 'token': 2697, 'token_str': '\u611f' } , { 'sequence': '[CLS] \u4eca \u5929 \u7231 \u60c5 \u5f88 \u597d [SEP]', 'score': 0.1470295488834381, 'token': 4263, 'token_str': '\u7231' } ' [ CLS ] \u4eca \u5929 \u8868 \u60c5 \u5f88 \u597d [ SEP ] ', 'score': 0.0740746483206749 , 'token': 6134 , 'token_str': '\u8868'}, { 'sequence': '[CLS] \u4eca \u5929 \u5fc3 \u60c5 \u5f88 \u597d [SEP]', 'score': 0.06646344810724258, 'token': 2552, 'token_str': '\u5fc3' } , { 'sequence': '[CLS] \u4eca ', 'score': 0.02915295772254467, 'token': 4178, 'token_str': '\u70ed' } ]]}","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#overview","text":"$ nlp2go arguments: either json config or model path --model model path or --json json file include models setting optional arguments: -h, --help show this help message and exit --enable_arg_panel enable argument input panel --path restful api path --port restful api hosting port --cli enable command line interface(default using restful api)","title":"Overview"},{"location":"usage/#load-model","text":"","title":"Load Model"},{"location":"usage/#load-tfkit-models","text":"","title":"load TFkit models"},{"location":"usage/#online-model","text":"nlp2go --model model_name_from_models_hub --cli","title":"online model"},{"location":"usage/#local-model","text":"nlp2go --model model_path.pt --cli","title":"local model"},{"location":"usage/#load-huggingfaces-transformers-models","text":"","title":"load Huggingface's transformers models"},{"location":"usage/#online-model_1","text":"nlp2go --model model_name_from_https://huggingface.co --task any of it:[feature-extraction, sentiment-analysis, ner, question-answering, fill-mask, summarization, translation_en_to_fr, translation_en_to_de, translation_en_to_ro, text-generation] --cli example: nlp2go --model voidful/albert_chinese_tiny --task fill-mask --cli","title":"online model"},{"location":"usage/#local-model_1","text":"nlp2go --model model_dir --task any of it:[feature-extraction, sentiment-analysis, ner, question-answering, fill-mask, summarization, translation_en_to_fr, translation_en_to_de, translation_en_to_ro, text-generation] --cli","title":"local model"},{"location":"usage/#load-multiple-models","text":"For deploying multiple model to restful api, nlp2go needs to load models from a json config file. nlp2go --json json_config_path --cli json format: { \"API1_PATH\" : { \"model\" : \"model1_path or name\" , \"model_parameter\" : \"argument\" }, \"API2_PATH\" : { \"model\" : \"model1_path or name\" , \"model_parameter\" : \"argument\" } } Example: ./test_conf.json { \"albert_mask\" : { \"model\" : \"voidful/albert_chinese_tiny\" , \"task\" : \"fill-mask\" } } run nlp2go --json ./test_conf.json --api_port 3000 result hosting api in path: /api/+ ['albert_mask'] Model loaded, serving demo on port 3000 test Get: localhost:3000/api/albert_mask?input=\u4eca\u5929[MASK]\u60c5\u5f88\u597d","title":"load multiple models"},{"location":"usage/#pre-load-models","text":"nlp2go-preload is to pre-download model files when we host our models using docker. that can avoid downloading model at run time nlp2go-preload --json config.json example dockerfile FROM pytorch/pytorch:latest RUN apt-get -y update # Create app directory RUN mkdir -p /usr/src/app WORKDIR /usr/src/app # Copy source project COPY . /usr/src/app/ RUN pip install nlprep tfkit nlp2go -U RUN nlp2go-preload --json config.json CMD PYTHONIOENCODING = utf8 nlp2go --json config.json --port ${ PORT }","title":"pre-load models"},{"location":"usage/#model-interface","text":"","title":"Model Interface"},{"location":"usage/#command-line-interface","text":"model predict using command line interface(cli) nlp2go --model voidful/albert_chinese_tiny --task fill-mask --cli argument: --enable_arg_panel to enable a input panel that let you input argument","title":"Command Line Interface"},{"location":"usage/#restful-api","text":"nlp2go --model voidful/albert_chinese_tiny --task fill-mask --api_path model --api_port 3000 argument: --enable_arg_panel to enable a input panel that let you input argument --api_path path set API path after 'host/API/' --api_port set hosting port","title":"Restful API"},{"location":"usage/#python-code","text":"import nlp2go albert_fillmask = nlp2go . Model ( model_path = \"voidful/albert_chinese_tiny\" , model_task = \"fill-mask\" ) albert_fillmask . predict ({ \"input\" : \"\u4eca\u5929[MASK]\u60c5\u5f88\u597d\" }) result { 'result': [[{'sequence': '[CLS] \u4eca \u5929 \u611f \u60c5 \u5f88 \u597d [SEP]', 'score': 0.40312328934669495, 'token': 2697, 'token_str': '\u611f' } , { 'sequence': '[CLS] \u4eca \u5929 \u7231 \u60c5 \u5f88 \u597d [SEP]', 'score': 0.1470295488834381, 'token': 4263, 'token_str': '\u7231' } ' [ CLS ] \u4eca \u5929 \u8868 \u60c5 \u5f88 \u597d [ SEP ] ', 'score': 0.0740746483206749 , 'token': 6134 , 'token_str': '\u8868'}, { 'sequence': '[CLS] \u4eca \u5929 \u5fc3 \u60c5 \u5f88 \u597d [SEP]', 'score': 0.06646344810724258, 'token': 2552, 'token_str': '\u5fc3' } , { 'sequence': '[CLS] \u4eca ', 'score': 0.02915295772254467, 'token': 4178, 'token_str': '\u70ed' } ]]}","title":"Python code"}]}